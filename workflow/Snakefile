from os.path import join
import pandas as pd
from collections import defaultdict
import yaml

#snakemake config
source_dir = config['source_dir']
out_dir = config['output_dir'].rstrip('/') + '/'
fastq_dir = config['fastq_dir'].rstrip('/') + '/'
talon_dir = config['talon'].rstrip('/') + '/'
cupcake_dir = config['cupcake'].rstrip('/') + '/'
sample_manifest=config['sample_manifest']
sqanti_yaml=config['sqanti_yaml']

#user params
annotation_id=config['annotation_id']
build_id=config['build_id']
maxFracA=config['maxFracA']
minCount=config['minCount']
minDatasets=config['minDatasets']
sequence_id=config['sequence_id']
CAGE=config['CAGE']

#ref information
anno_gtf=config['annotation_gtf']
anno_fa=config['annotation_fa']
anno_gff=config['annotation_gff']

#barcode list
df_sample = pd.read_csv(sample_manifest,sep="\t")
bc_list = df_sample['filename']

#talon config
def talon_config(wildcards):
    
    #create array for config
    config_data = []

    #for each of the barcodes, generate config info
    #example: SIRV_Rep1,SIRV,PacBio-Sequel2,/data/sevillas2/RBL3/tutorial/labeled/SIRV_rep1_labeled.sam
    for bc in bc_list:
        df_sub = df_sample[df_sample["filename"] == bc]
        
        output_filename = join(out_dir,'labeled',bc + '_labeled.sam')
        config_data.append(df_sub.iloc[0]['sampleid'] + "," + build_id + "," + sequence_id + "," + output_filename)
    
    #create config
    talon_config = join(out_dir,'talon', 'talon_config.csv')
    with open(talon_config, "w") as txt_file:
        for line in config_data:
            txt_file.write(line + "\n")


rule all:
    input:
        #input fastq files
        expand(join(fastq_dir,'{bc}.fastq'),bc=bc_list),
        expand(join(out_dir,'fastq','{bc}.R1.fastq.gz'),bc=bc_list),

        #sam files
        expand(join(out_dir,'sam','{bc}.sam'),bc=bc_list),
        expand(join(out_dir,'sam','{bc}_sorted.sam'),bc=bc_list),

        #talon
        join(out_dir,'talon', 'talon_config.csv'),
        join(out_dir,'talon', build_id + '.db'),
        expand(join(out_dir,'labeled','{bc}_labeled.sam'),bc=bc_list),
        join(out_dir,'talon', build_id + '_talon_read_annot.tsv'),
        join(out_dir,'talon', build_id + '_talon_summary.tsv'),
        #join(out_dir,'counts', build_id + '_talon_abundance.txt'),
        #join(out_dir,'counts', build_id + '_whitelist.txt'),
        #join(out_dir,'counts', build_id + '_talon_abundance_filtered.txt'),
        #join(out_dir,'gtf', build_id + '_talon.gtf')


        #collapsed files
        #expand(join(out_dir,'collapsed','{bc}.collapsed.gff'),bc=bc_list),
        #expand(join(out_dir,'collapsed','{bc}.collapsed.rep.fq'),bc=bc_list),
        #expand(join(out_dir,'collapsed','{bc}.collapsed.group.txt'),bc=bc_list),
        #expand(join(out_dir,'collapsed','{bc}.ignored_ids.txt'),bc=bc_list),

rule handle_fastq:
    '''
    move and zip fastq files 
    '''
    input:
        f1 = join(fastq_dir,'{bc}.fastq')
    params:
        rname = "01_fq",
        base = join(out_dir,'fastq','{bc}.R1.fastq')
    output:
        o1 = join(out_dir,'fastq','{bc}.R1.fastq.gz')
    shell:
        '''
        cp {input.f1} {output.o1}; \
        gzip {output.o1}
        '''

rule create_sam:
    '''
    # cupcake tutorial
    #https://github.com/Magdoll/cDNA_Cupcake/wiki/Cupcake:-supporting-scripts-for-Iso-Seq-after-clustering-step
    '''
    input:
        f1 = join(out_dir,'fastq','{bc}.R1.fastq.gz')
    params:
        rname = "02_create_sam",
        fa = anno_fa
    envmodules:
        config['minimap2'],
    output:
        sam = join(out_dir,'sam','{bc}.sam'),
        sam_sort = join(out_dir,'sam','{bc}_sorted.sam')
    shell:
        '''
        minimap2 \
            -ax splice -t 30 -uf --secondary=no -C5 --MD \
            {params.fa} {input.f1} > {output.sam};
        sort -k 3,3 -k 4,4n {output.sam} > {output.sam_sort}
        '''

rule talon_config:
    '''
    generate talon config
    '''
    input:
        f1 = expand(join(out_dir,'fastq','{bc}.R1.fastq.gz'),bc=bc_list),
    params:
        rname = "02_talon_config",
        process = talon_config
    output:
        o1 = join(out_dir,'talon','talon_config.csv')

rule talon_db:
    '''
    initialize db
    '''
    input:
        anno = anno_gtf,
    params:
        rname = "03_talon_db",
        script_dir = talon_dir,
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'talon',build_id)
    envmodules:
        config['python'],
    output:
        o1 = join(out_dir,'talon',build_id + '.db')
    shell:
        '''
        pip install {params.script_dir};
        talon_initialize_database \
            --f {input.anno} \
            --a {params.a_id}\
            --g {params.b_id} \
            --o {params.base}
        '''

rule talon_prime:
    '''
    interal primiing - how likely each read si will be used as an internal priming product
    labeling only - no reads are removed

    tmp_dir needs to be unique to sample; otherwise run will fail as it attempts to write over
    files with each sample
    --deleteTmp
    '''
    input:
        f1 = join(out_dir,'sam','{bc}.sam'),
    params:
        rname = "04_talon_prime",
        script_dir = talon_dir,
        anno = anno_fa,
        base_tmp = join(out_dir,'tmp' + '_' + '{bc}'),
        base_sample = join(out_dir,'labeled','{bc}')
    envmodules:
        config['python'],
        config['bedtools']
    output:
        o1 = join(out_dir,'labeled','{bc}_labeled.sam'),
        o2 = join(out_dir,'labeled','{bc}_read_labels.tsv'),
    shell:
        '''
        pip install {params.script_dir} ;
        talon_label_reads --f {input.f1} \
            --g {params.anno} \
            --t 1 \
            --ar 20 \
            --tmpDir={params.base_tmp} \
            --deleteTmp \
            --o {params.base_sample}
        '''

rule talon_annotation:
    '''
    annotate and quantify reads; modify db
    '''
    input:
        db = join(out_dir, 'talon', build_id + '.db'),
        t_config = join(out_dir, 'talon', 'talon_config.csv')
    params:
        rname = "05_talon_anno",
        script_dir = talon_dir,
        b_id = build_id,
        base = join(out_dir,'talon',build_id)
    envmodules:
        config['python'],
    output:
        o1 = join(out_dir,'talon', build_id + '_talon_read_annot.tsv'),
        o2 = join(out_dir,'talon', build_id + '_QC.log'),
    shell:
        '''
        rm -rf talon_tmp; 
        pip install {params.script_dir};
        talon \
            --f {input.t_config} \
            --db {input.db} \
            --build {params.b_id} \
            --t 5 \
            --o {params.base}
        '''

rule talon_summary:
    '''
    summarize how many transcripts before filtering
    '''
    input:
        db = join(out_dir,'talon',build_id + '.db'),
        anno = join(out_dir,'talon', build_id + '_talon_read_annot.tsv'),
    params:
        rname = "06_talon_summary",
        script_dir = talon_dir,
        base = join(out_dir,'talon', build_id)
    envmodules:
        config['python'],
    output:
        o1 = join(out_dir,'talon', build_id + '_talon_summary.tsv')
    shell:
        '''
        pip install {params.script_dir};
        talon_summarize --db {input.db} --v --o {params.base}
        '''

rule talon_counts:
    '''
    abundance matrix (for comp gene expression) without filtering
    '''
    input:
        db = join(out_dir,'talon',build_id + '.db'),
    params:
        rname = "07_talon_counts",
        script_dir = talon_dir,
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'counts', build_id)
    envmodules:
        config['python'],
    output:
        o1 = join(out_dir,'counts', build_id + '_talon_abundance.txt')
    shell:
        '''
        pip install {params.script_dir};
        talon_abundance --db {input.db} -a {params.a_id} --build {params.b_id} --o {params.base}
        '''

rule talon_whitelist:
    '''
    repeat with TALON filters
    '''
    input:
        db = join(out_dir,'talon',build_id + '.db'),
    params:
        rname = "08_talon_wl",
        script_dir = talon_dir,
        a_id = annotation_id,
        max_frac  = maxFracA,
        min_count = minCount,
        min_ds = minDatasets
    envmodules:
        config['python'],
    output:
        o1 = join(out_dir,'counts',build_id + '_whitelist.txt')
    shell:
        '''
        pip install {params.script_dir};
        talon_filter_transcripts \
            --db {input.db} \
            -a {params.a_id} \
            --maxFracA {params.max_frac} \
            --minCount {params.min_count} \
            --minDatasets {params.min_ds} \
            --o {output.o1}
        '''

rule talon_abundance_filtered:
    '''
    abundance matrix (for comp gene expression) with filtering
    '''
    input:
        db = join(out_dir,'talon',build_id + '.db'),
        white_list = join(out_dir,'counts',build_id + '_whitelist.txt')
    params:
        rname = "09_talon_counts",
        script_dir = talon_dir,
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'counts', build_id)
    envmodules:
        config['python'],
    output:
        o1 = join(out_dir,'counts', build_id + '_talon_abundance_filtered.txt')
    shell:
        '''
        pip install {params.script_dir};
        talon_abundance \
            --db {input.db} \
            -a {params.a_id} \
            --whitelist {input.white_list} \
            --build {params.b_id} \
            --o {params.base}
        '''

rule talon_gtf:
    '''
    create custom GTF of filtered transcripts
    '''
    input:
        db = join(out_dir,'talon',build_id + '.db'),
        w_list = join(out_dir,'counts',build_id + '_whitelist.txt')
    params:
        rname = "10_talon_gtf",
        script_dir = talon_dir,
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'gtf', build_id)
    envmodules:
        config['python'],
    output:
        o1 = join(out_dir,'gtf', build_id + '_talon.gtf')
    shell:
        '''
        pip install {params.script_dir};
        talon_create_GTF \
            --db {input.db} \
            --whitelist {input.w_list} \
            -a {params.a_id} \
            --build {params.b_id} \
            --o {params.base}
        '''

rule collapse:
    '''
    # cupcake tutorial
    #https://github.com/Magdoll/cDNA_Cupcake/wiki/Cupcake:-supporting-scripts-for-Iso-Seq-after-clustering-step
    '''
    input:
        f1 = join(out_dir,'fastq','{bc}.R1.fastq.gz'),
        sam = join(out_dir,'sam','{bc}_sorted.sam')
    params:
        rname = "create_sam",
        bc = '{bc}',
        script_dir = cupcake_dir,
        py = join(source_dir,'dependencies/cDNA_Cupcake/cupcake/tofu/','collapse_isoforms_by_sam.py')
    envmodules:
        config['python'],
    output:
        o1 = join(out_dir,'collapsed','{bc}.collapsed.gff'),
        o2 = join(out_dir,'collapsed','{bc}.collapsed.rep.fq'),
        o3 = join(out_dir,'collapsed','{bc}.collapsed.group.txt'),
        o4 = join(out_dir,'collapsed','{bc}.ignored_ids.txt'),
    shell:
        '''
        pip install {params.script_dir};
        python {params.py} \
            --input {input.f1} \
            --fq \
            -s {input.sam} \
            --dun-merge-5-shorter \
            -o {params.bc}
        '''

rule squanti:
    '''
    remove params: 
    --polyA_motif_list  polyA.list

    for short read data only:
    --expression rsemQuantification.chr13.isoforms.results
    -c star.SJ.out.tab \

    '''
    input:
        gtf = join(out_dir,'gtf', build_id + '_talon.gtf'),
        counts = join(out_dir,'counts', build_id + '_talon_abundance_filtered.txt')
    params:
        rname = "11_sqanti",
        gtf = anno_gtf,
        fa = anno_fa,
        cage = CAGE,
        gff = anno_gff,
    envmodules:
        config['python'],
    conda:
        config['sqanti_yaml']
    output:
        o1 = join(out_dir,'tbd')
    shell:
        '''
        python sqanti3_qc.py \
            --gtf {input.gtf} {params.gtf} {params.fa} \
            --cage_peak {params.cage} \
            --fl_count {input.counts} \
            --isoAnnotLite \
            --gff3 {params.gff}
        '''


'''
rule :
    input:
        f1 = 
    params:
        rname = "create_sam",
    envmodules:
        config[''],
    output:
        o1 = 
    shell:
'''