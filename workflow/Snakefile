from os.path import join
import pandas as pd
from collections import defaultdict
import yaml

#config dir
source_dir = config['source_dir']
out_dir = config['output_dir'].rstrip('/') + '/'
fastq_dir = config['fastq_dir'].rstrip('/') + '/'
talon_conda = config['talon_conda']
sample_manifest=config['sample_manifest']
deg_manifest = config['deg_manifest']

porechop_conda = config['porechop_conda']
flair_conda = config['flair_conda']

#config user params
annotation_id=config['annotation_id']
build_id=config['build_id']
maxFracA=config['maxFracA']
minCount=config['minCount']
minDatasets=config['minDatasets']
platform_id=config['platform_id']
primer_len=config['primer_length']
perc_sim = config['percent_similarity']
num_match = config['number_of_matches']

#ref information
anno_gtf=config['annotation_gtf']
anno_fa=config['annotation_fa']
anno_gff=config['annotation_gff']

#barcode list
df_sample = pd.read_csv(sample_manifest,sep="\t")
bc_list = df_sample['filename']

#talon config
def talon_config(wildcards):
    
    #create array for config
    config_data = []

    #for each of the barcodes, generate config info
    #example: SIRV_Rep1,SIRV,PacBio-Sequel2,/data/sevillas2/RBL3/tutorial/labeled/SIRV_rep1_labeled.sam
    for bc in bc_list:
        df_sub = df_sample[df_sample["filename"] == bc]
        
        output_filename = join(out_dir,'04_sam_labeled',bc + '_labeled.sam')
        config_data.append(df_sub.iloc[0]['sampleid'] + "," + build_id + "," + platform_id + "," + output_filename)
    
    #create config
    talon_config = join(out_dir,'03_talon', 'talon_config.csv')
    with open(talon_config, "w") as txt_file:
        for line in config_data:
            txt_file.write(line + "\n")

#flair config
def flair_config(wildcards):
    
    #create array for config
    config_data = []

    #for each of the barcodes, generate config info
    #example: wt1	wt	b1	/scratch/kopardevn/rbl3_test_out/fastq/barcode01.trimmed.R1.fastq.gz
    for bc in bc_list:
        df_sub = df_sample[df_sample["filename"] == bc]
        
        fq_name = join(fastq_dir,'01_fastq',bc + '_trimmed.fastq')
        config_data.append(df_sub.iloc[0]['sampleid'] + "\t" + df_sub.iloc[0]['groupid'] + "\t" + "b1" + "\t" + fq_name)
    
    #create config
    flair_config = join(out_dir,'09_flair', 'flair_config.csv')
    with open(flair_config, "w") as txt_file:
        for line in config_data:
            txt_file.write(line + "\n")

#deg manifest
df_deg = pd.read_csv(deg_manifest,sep=',')
group_list = df_deg['group2']

#local rules
localrules: talon_config, flair_config

rule all:
    input:
        #input fastq files
        expand(join(fastq_dir,'{bc}.fastq'),bc=bc_list),
        expand(join(out_dir,'01_fastq','{bc}.fastq.gz'),bc=bc_list),
        expand(join(out_dir,'01_fastq','{bc}_trimmed.fastq.gz'),bc=bc_list),

        #sam files
        expand(join(out_dir,'02_sam','{bc}_sorted.sam'),bc=bc_list),
        expand(join(out_dir,'02_sam_corrected','{bc}_sorted.sam'),bc=bc_list),

        #bam files
        expand(join(out_dir,'03_bam','{bc}.bam'),bc=bc_list),

        #qc
        expand(join(out_dir, '00_qc','fastqc','{bc}.R1_fastqc.html'),bc=bc_list),
        expand(join(out_dir, '00_qc','samtools','{bc}_samstats.txt'),bc=bc_list),
        join(out_dir,'00_qc','multiqc_report.html'),
        join(out_dir,'00_qc','qc_report.html'),

        #talon
        join(out_dir,'03_talon', 'talon_config.csv'),
        join(out_dir,'03_talon', build_id + '.db'),
        expand(join(out_dir,'04_sam_labeled','{bc}_labeled.sam'),bc=bc_list),
        join(out_dir,'05_annotation', build_id + '_talon_read_annot.tsv'),
        join(out_dir,'06_read_counts', build_id + '_talon_summary.tsv'),
        join(out_dir,'06_read_counts', build_id + '_talon_abundance.tsv'),
        join(out_dir,'06_read_counts', build_id + '_whitelist.txt'),
        join(out_dir,'06_read_counts', build_id + '_talon_abundance_filtered.tsv'),
        join(out_dir,'07_gtf', build_id + '_talon.gtf'),
        join(out_dir,'08_transcript_plots','abundance_plots.html'),

        #flair
        join(out_dir,'01_fastq','merged.fastq.gz'),
        join(out_dir,'09_flair','merged_flair.isoforms.fa'),
        join(out_dir,'09_talon','flair_config.csv'),
        join(out_dir,'09_flair','counts_matrix.tsv'),
        #expand(join(out_dir,'09_flair','diff_iso_{group_id}.txt'),group_id=group_list),

        # #squanti
        # #join(out_dir,'tbd'),

        #collapsed files
        #expand(join(out_dir,'collapsed','{bc}.collapsed.gff'),bc=bc_list),
        #expand(join(out_dir,'collapsed','{bc}.collapsed.rep.fq'),bc=bc_list),
        #expand(join(out_dir,'collapsed','{bc}.collapsed.group.txt'),bc=bc_list),
        #expand(join(out_dir,'collapsed','{bc}.ignored_ids.txt'),bc=bc_list),

rule handle_fastq:
    '''
    move and zip fastq files 
    '''
    input:
        f1 = join(fastq_dir,'{bc}.fastq')
    params:
        rname = "01_fq",
        base = join(out_dir,'01_fastq','{bc}.fastq'),
        unzip = join(out_dir,'01_fastq','{bc}.fastq'),
    output:
        zip = join(out_dir,'01_fastq','{bc}.fastq.gz')
    shell:
        '''
        cp {input.f1} {params.unzip}; \
        gzip {params.unzip}
        '''

rule adaptor_trim:
    '''
    adapter trimming
    '''
    input:
        f1 = join(out_dir,'01_fastq','{bc}.fastq.gz')
    params:
        rname = "trim_adaptors",
        doc = porechop_conda,
    envmodules:
        config['singularity'],
    output:
        o1 = join(out_dir,'01_fastq','{bc}_trimmed.fastq.gz')
    shell:
        '''
        singularity shell -B /data/$USER,/data/RBL_NCI,{out_dir},/fdb,/scratch \
            {params.doc} porechop -i {input.f1} -o {output.o1} -t 2
        '''

rule create_sam:
    '''
    # cupcake tutorial
    #https://github.com/Magdoll/cDNA_Cupcake/wiki/Cupcake:-supporting-scripts-for-Iso-Seq-after-clustering-step

    minimap flags
    https://lh3.github.io/minimap2/minimap2.html
    '''
    input:
        f1 = join(out_dir,'01_fastq','{bc}_trimmed.fastq.gz')
    params:
        rname = "02_sam",
        fa = anno_fa
    envmodules:
        config['minimap2'],
        config['samtools']
    output:
        sam = join(out_dir,'02_sam','{bc}.sam'),
        sorted = join(out_dir,'02_sam','{bc}_sorted.sam')
    shell:
        '''
        minimap2 \
            -ax splice -t 30 --secondary=no --MD \
            {params.fa} {input.f1} > {output.sam};
        samtools sort {output.sam} -o {output.sorted}
        '''

rule clean_sam:
    input:
        f1 = join(out_dir,'02_sam','{bc}_sorted.sam')
    params:
        rname = "clean_sam",
        anno_fa = anno_fa,
        base = join(out_dir,'02_sam_corrected','{bc}')
    container: 
        talon_conda
    output:
        o1 = join(out_dir,'02_sam_corrected','{bc}_sorted.sam')
    shell:
        '''
        TranscriptClean.py --sam {input.f1} --genome {params.anno_fa} --outprefix {params.base} --deleteTmp
        '''

rule create_bam:
    input:
        f1 = join(out_dir,'02_sam_corrected','{bc}_sorted.sam')
    params:
        rname='03_bam'
    envmodules:
        config['samtools']
    output:
        bam = join(out_dir,'03_bam','{bc}.bam'),
        sorted = join(out_dir, '03_bam','{bc}_sorted.bam')
    shell:
        """
        samtools view -bS {input.f1} -o {output.bam}; \
        samtools sort {output.bam} -o {output.sorted}; \
        samtools index {output.sorted}; 
        """

rule qc_fastq:
    """
    Runs FastQC report on each sample 
    """
    input:
        f1 = join(out_dir,'01_fastq','{bc}_trimmed.fastq.gz')
    params:
        rname='03_fqc',
        base = join(out_dir, '00_qc','fastqc'),
    envmodules:
        config['fastqc']
    output:
        o1 = join(out_dir, '00_qc','fastqc','{bc}.R1_fastqc.html')
    shell:
        """
        fastqc {input.f1} -o {params.base}
        """

rule qc_samstats:
    """
    generate statistics for sam file before deduplication
    http://www.htslib.org/doc/samtools-stats.html
    > $1
    """
    input:
        f1 = join(out_dir, '03_bam','{bc}_sorted.bam')
    params:
        rname='03_samstats'
    envmodules:
        config['samtools']
    output:
        o1 = join(out_dir, '00_qc','samtools','{bc}_samstats.txt')
    shell:
        """
        samtools view -h {input.f1} | samtools stats - > {output.o1}
        """

rule qc_multiqc:
    """
    merges FastQC reports for pre/post trimmed fastq files into MultiQC report
    https://multiqc.info/docs/#running-multiqc
    """
    input:
        f1 = expand(join(out_dir, '00_qc', 'fastqc','{bc}.R1_fastqc.html'),bc=bc_list),
        f2 = expand(join(out_dir, '00_qc', 'samtools','{bc}_samstats.txt'),bc=bc_list)
    params:
        rname = '03_multiqc',
        out = join(out_dir,'00_qc'),
        qc_config = join(source_dir,'config','multiqc_config.yaml'),
        dir_fastq = expand(join(out_dir, '00_qc', 'fastqc')),
        dir_screen_species = expand(join(out_dir, '00_qc', 'screen')),
    envmodules:
        config['multiqc']
    output:
        o1 = join(out_dir,'00_qc','multiqc_report.html')
    shell:
        """
        multiqc -f -v -c {params.qc_config} \
            -d -dd 1 {params.dir_fastq} {params.dir_screen_species} \
            -o {params.out}
        """

rule qc_alignment:
    """
    uses samtools to create a bams of unaligned reads and aligned reads
    input; print qlength col to text file
    generates plots and summmary file for aligned vs unaligned statistics
    """
    input:
        f1 = join(out_dir, '03_bam','{bc}_sorted.bam')
    params:
        rname = "03_qc_align",
        R = join(source_dir,'workflow','scripts','alignment_stats.R'),
        base = join(out_dir, '00_qc', 'alignment/')
    envmodules:
        config['samtools'],
        config['R']
    output:
        bam_a = join(out_dir, '00_qc', 'alignment','{bc}_align_len.txt'),
        bam_u = join(out_dir, '00_qc', 'alignment','{bc}_unalign_len.txt'),
        png_align = join(out_dir, '00_qc', 'alignment','{bc}_aligned.png'),
        png_unalign = join(out_dir, '00_qc', 'alignment','{bc}_unaligned.png'),
        txt_align = join(out_dir, '00_qc', 'alignment','{bc}_aligned.txt'),
        txt_unalign = join(out_dir, '00_qc', 'alignment','{bc}_unaligned.txt'),
    shell:
        """
        samtools view -F 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_a}; \
        samtools view -f 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_u}; \
        Rscript {params.R} {wildcards.bc} {output.bam_a} {output.bam_u} {params.base}
        """

rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, '00_qc', 'alignment','{bc}_aligned.png'), bc=bc_list),
            png_unalign = expand(join(out_dir, '00_qc', 'alignment','{bc}_unaligned.png'), bc=bc_list),
            txt_align = expand(join(out_dir, '00_qc', 'alignment','{bc}_aligned.txt'), bc=bc_list),
            txt_unalign = expand(join(out_dir, '00_qc', 'alignment','{bc}_unaligned.txt'), bc=bc_list),
        params:
            rname = "03_qc_ts",
            R = join(source_dir,'workflow','scripts','qc_report_nondemux.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'00_qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{input.txt_align}", \
                    u_txt = "{input.txt_unalign}"))'
            """
            
rule talon_config:
    '''
    generate talon config
    '''
    input:
        f1 = expand(join(out_dir,'01_fastq','{bc}_trimmed.fastq.gz'),bc=bc_list),
    params:
        rname = "talon_config",
        process = talon_config
    output:
        o1 = join(out_dir,'03_talon','talon_config.csv')

rule talon_db:
    '''
    initialize db
    '''
    input:
        anno = anno_gtf,
    params:
        rname = "03_talon_db",
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'03_talon',build_id)
    output:
        o1 = join(out_dir,'03_talon',build_id + '.db')
    container: 
        talon_conda
    shell:
        '''
        talon_initialize_database \
            --f {input.anno} \
            --a {params.a_id}\
            --g {params.b_id} \
            --o {params.base}
        '''

rule talon_prime:
    '''
    Current long-read platforms that rely on poly-(A) selection are prone to internal priming artifacts. 
    These occur when the oligo-dT primer binds off-target to A-rich sequences inside an RNA transcript 
    rather than at the end. 
    
    Records the fraction of As in the n-sized window immediately following each read alignment; output SAM 
    file with the fraction recorded in the fA:f custom SAM tag
    
    tmp_dir needs to be unique to sample; otherwise run will fail as it attempts to write over
    files with each sample
    --deleteTmp
    '''
    input:
        f1 = join(out_dir,'02_sam_corrected','{bc}_sorted.sam'),
    params:
        rname = "04_label",
        anno = anno_fa,
        base_tmp = join(out_dir,'tmp' + '_' + '{bc}'),
        base_sample = join(out_dir,'04_sam_labeled','{bc}'),
        p_len = primer_len
    output:
        o1 = join(out_dir,'04_sam_labeled','{bc}_labeled.sam'),
        o2 = join(out_dir,'04_sam_labeled','{bc}_read_labels.tsv'),
    container: 
        talon_conda
    shell:
        '''
        talon_label_reads --f {input.f1} \
            --g {params.anno} \
            --t 1 \
            --ar {params.p_len} \
            --tmpDir={params.base_tmp} \
            --deleteTmp \
            --o {params.base_sample}
        '''

rule talon_annotation:
    '''
    annotate and quantify reads; modify db
    '''
    input:
        db = join(out_dir, '03_talon', build_id + '.db'),
        t_config = join(out_dir, '03_talon', 'talon_config.csv'),
        sam = expand(join(out_dir,'04_sam_labeled','{bc}_labeled.sam'),bc=bc_list)
    params:
        rname = "05_annotate",
        b_id = build_id,
        base_tmp = join(out_dir,'tmp_annotations/'),
        base_sample = join(out_dir,'05_annotation',build_id)
    output:
        o1 = join(out_dir,'05_annotation', build_id + '_talon_read_annot.tsv'),
        o2 = join(out_dir,'05_annotation', build_id + '_QC.log'),
    container: 
        talon_conda
    shell:
        '''
        talon \
            --f {input.t_config} \
            --db {input.db} \
            --build {params.b_id} \
            --threads 5 \
            --tmp_dir {params.base_tmp} \
            --o {params.base_sample}
        '''

rule talon_summary:
    '''
    summarize how many transcripts before filtering
    '''
    input:
        db = join(out_dir,'03_talon',build_id + '.db'),
        anno = join(out_dir,'05_annotation', build_id + '_talon_read_annot.tsv'),
    params:
        rname = "06.1_summary",
        base = join(out_dir,'06_read_counts', build_id)
    output:
        o1 = join(out_dir,'06_read_counts', build_id + '_talon_summary.tsv')
    container: 
        talon_conda
    shell:
        '''
        talon_summarize --db {input.db} --v --o {params.base}
        '''

rule talon_counts:
    '''
    abundance matrix (for comp gene expression) without filtering
    '''
    input:
        db = join(out_dir,'03_talon',build_id + '.db'),
        anno = join(out_dir,'05_annotation', build_id + '_talon_read_annot.tsv'),
    params:
        rname = "06.2_abundance",
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'06_read_counts', build_id)
    output:
        o1 = join(out_dir,'06_read_counts', build_id + '_talon_abundance.tsv')
    container: 
        talon_conda
    shell:
        '''
        talon_abundance --db {input.db} -a {params.a_id} --build {params.b_id} --o {params.base}
        '''

rule talon_whitelist:
    '''
    repeat with TALON filters
    '''
    input:
        db = join(out_dir,'03_talon',build_id + '.db'),
        anno = join(out_dir,'05_annotation', build_id + '_talon_read_annot.tsv'),
    params:
        rname = "06.3_whitelist",
        a_id = annotation_id,
        max_frac  = maxFracA,
        min_count = minCount,
        min_ds = minDatasets
    output:
        o1 = join(out_dir,'06_read_counts',build_id + '_whitelist.txt')
    container: 
        talon_conda
    shell:
        '''
        talon_filter_transcripts \
            --db {input.db} \
            -a {params.a_id} \
            --maxFracA {params.max_frac} \
            --minCount {params.min_count} \
            --minDatasets {params.min_ds} \
            --o {output.o1}
        '''

rule talon_abundance_filtered:
    '''
    abundance matrix (for comp gene expression) with filtering
    '''
    input:
        db = join(out_dir,'03_talon',build_id + '.db'),
        white_list = join(out_dir,'06_read_counts',build_id + '_whitelist.txt')
    params:
        rname = "06.4_abundance_filt",
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'06_read_counts', build_id)
    output:
        o1 = join(out_dir,'06_read_counts', build_id + '_talon_abundance_filtered.tsv')
    container: 
        talon_conda
    shell:
        '''
        talon_abundance \
            --db {input.db} \
            -a {params.a_id} \
            --whitelist {input.white_list} \
            --build {params.b_id} \
            --o {params.base}
        '''

rule talon_gtf:
    '''
    create custom GTF of filtered transcripts
    '''
    input:
        db = join(out_dir,'03_talon',build_id + '.db'),
        w_list = join(out_dir,'06_read_counts',build_id + '_whitelist.txt')
    params:
        rname = "07_gtf",
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'07_gtf', build_id)
    output:
        o1 = join(out_dir,'07_gtf', build_id + '_talon.gtf')
    container: 
        talon_conda
    shell:
        '''
        talon_create_GTF \
            --db {input.db} \
            --whitelist {input.w_list} \
            -a {params.a_id} \
            --build {params.b_id} \
            --o {params.base}
        '''

rule abundance_plots:
    input:
        unfilt = join(out_dir,'06_read_counts', build_id + '_talon_abundance.tsv'),
        filt = join(out_dir,'06_read_counts', build_id + '_talon_abundance_filtered.tsv')
    params:
        rname = "08_abundance_plots",
        R = join(source_dir,"workflow","scripts","transcript_types.Rmd"),
        base = join(out_dir,'08_transcript_plots'),
        perc_sim = perc_sim,
        num_match = num_match
    envmodules:
        config['R'],
    output:
        o1 = join(out_dir,'08_transcript_plots','abundance_plots.html')
    shell:
        '''
        Rscript -e 'library(rmarkdown); \
        rmarkdown::render("{params.R}",
            output_file = "{output.o1}", \
            params= list(f_data = "{input.filt}", \
                u_data = "{input.unfilt}", \
                output_dir = "{params.base}", \
                perc_sim = "{params.perc_sim}", \
                num_match = "{params.num_match}"))'
        '''

rule merge_fq:
    '''
    '''
    input:
        fqs = expand(join(out_dir,'01_fastq','{bc}_trimmed.fastq.gz'),bc=bc_list)
    params:
        rname = "create_sam",
        base = join(fastq_dir,'01_fastq')
    output:
        o1 = join(out_dir,'01_fastq','merged.fastq.gz')
    shell:
        '''
        zcat {params.base}/*.gz | gzip -n -> {output.o1}
        '''

rule flair_isoforms:
    '''
    run flair to create isoforms
    
    singularity exec -B /scratch/kopardevn,/data/CCBR_Pipeliner /scratch/kopardevn/rbl3_test_out/ccbr_flair_v1.5.sif \
        flair.py 123 -g /data/CCBR_Pipeliner/db/PipeDB/Indices/hg38_basic/indexes/hg38.fa \
        -r  /scratch/kopardevn/rbl3_test_out/fastq/merged.fastq.gz \
        -f /data/CCBR_Pipeliner/db/PipeDB/Indices/GTFs/hg38/gencode.v30.annotation.gtf \
        -o /scratch/kopardevn/rbl3_test_out/fastq/merged.flair.output \
        --threads 16 \
        --temp_dir /scratch/kopardevn/rbl3_test_out/fastq/merged.flair.output.tmpdir
    '''
    input:
        f1 = join(out_dir,'01_fastq','merged.fastq.gz')
    params:
        rname = "flair_isoforms",
        doc = flair_conda,
        anno_fa = anno_fa,
        anno_gtf = anno_gtf,
        base = join(out_dir,'09_flair','merged_flair'),
        tmp_dir = join(out_dir,'09_flair','tmp_iso')
    envmodules:
        config['singularity'],
    output:
        o1 = join(out_dir,'09_flair','merged_flair.isoforms.fa')
    shell:
        '''
        singularity shell -B /data/$USER,/data/RBL_NCI,{out_dir},/fdb,/scratch \
            {params.doc} flair.py 123 -g {params.anno_fa} \
            -r  {input.f1} \
            -f {params.anno_gtf} \
            -o {params.base} \
            --threads 16 \
            --temp_dir {params.tmp_dir}
        '''

rule flair_config:
    '''
    generate talon config
    '''
    input:
        f1 = expand(join(out_dir,'01_fastq','{bc}_trimmed.fastq.gz'),bc=bc_list),
    params:
        rname = "flair_config",
        process = flair_config
    output:
        o1 = join(out_dir,'09_talon','flair_config.csv')

rule flair_abundances:
    '''
    ## quantify

    singularity exec -B /scratch/kopardevn,/data/CCBR_Pipeliner /scratch/kopardevn/rbl3_test_out/ccbr_flair_v1.5.sif \
    flair.py quantify \
    -r /scratch/kopardevn/rbl3_test_out/fastq/reads_manifest.tsv \
    --threads 16 \
    --tpm \
    -i /scratch/kopardevn/rbl3_test_out/fastq/merged.flair.output.isoforms.fa \
    --temp_dir /scratch/kopardevn/rbl3_test_out/fastq/flair.quantify.tmpdir
    '''
    input:
        config = join(out_dir,'09_talon','flair_config.csv'),
        iso = join(out_dir,'09_flair','merged_flair.isoforms.fa')
    params:
        rname = "flair_abundances",
        docs = flair_conda,
        tmp_dir = join(out_dir,'09_flair','tmp_abund'),
    envmodules:
        config['singularity'],
    output:
        o1 = join(out_dir,'09_flair','counts_matrix.tsv')
    shell:
        '''
        singularity shell -B /data/$USER,/data/RBL_NCI,{out_dir},/fdb,/scratch \
            {params.docs} flair.py quantify \
            -r {input.config} \
            --threads 16 \
            --tpm \
            -i {input.iso} \
            --temp_dir {params.tmp_dir}
        '''

# rule flair_deg:
#     '''
#     #using the diff_iso_usage.py script instead

#     singularity exec -B /scratch/kopardevn,/data/CCBR_Pipeliner /scratch/kopardevn/rbl3_test_out/ccbr_flair_v1.5.sif \
#     bash -c "python3 /opt2/flair/bin/diff_iso_usage.py /scratch/kopardevn/rbl3_test_out/fastq/counts_matrix.tsv wt1_wt_b1 ko_drosha1_ko_drosha_b1 /scratch/kopardevn/rbl3_test_out/fastq/diff_iso_ko_drosha.txt"
#     singularity exec -B /scratch/kopardevn,/data/CCBR_Pipeliner /scratch/kopardevn/rbl3_test_out/ccbr_flair_v1.5.sif \
#     bash -c "python3 /opt2/flair/bin/diff_iso_usage.py /scratch/kopardevn/rbl3_test_out/fastq/counts_matrix.tsv wt1_wt_b1 ko_dicer1_ko_dicer_b1 /scratch/kopardevn/rbl3_test_out/fastq/diff_iso_ko_dicer.txt"

#     '''
#     input:
#         f1 = join(out_dir,'09_flair','counts_matrix.tsv')
#     params:
#         rname = "create_sam",
#         dosc = flair_conda,
#         scripts = "/opt2/flair/bin/diff_iso_usage.py"
#     envmodules:
#         config['singularity'],
#     output:
#         o1 = join(out_dir,'09_flair','diff_iso_{group_id}.txt')
#     shell:
#     '''
#     singularity shell -B /data/$USER,/data/RBL_NCI,{out_dir},/fdb,/scratch \
#         {params.doc} bash -c "python3 /opt2/flair/bin/diff_iso_usage.py \
#         bash -c "python3 {params.scripts} {input.counts} wt1_wt_b1 ko_drosha1_ko_drosha_b1 /scratch/kopardevn/rbl3_test_out/fastq/diff_iso_ko_drosha.txt"
#         singularity exec -B /scratch/kopardevn,/data/CCBR_Pipeliner /scratch/kopardevn/rbl3_test_out/ccbr_flair_v1.5.sif \
#         bash -c "python3 /opt2/flair/bin/diff_iso_usage.py /scratch/kopardevn/rbl3_test_out/fastq/counts_matrix.tsv wt1_wt_b1 ko_dicer1_ko_dicer_b1 /scratch/kopardevn/rbl3_test_out/fastq/diff_iso_ko_dicer.txt"

#     '''


# rule squanti:
#     '''
#     remove params: 
#     --polyA_motif_list  polyA.list
#     --cage_peak {params.cage} \

#     for short read data only:
#     --expression rsemQuantification.chr13.isoforms.results
#     -c star.SJ.out.tab \
    
#     sqanti3_qc.py --gtf {input.gtf} {params.anno} --fl_count {input.counts} --isoAnnotLite --gff3 {params.gff}

#     '''
#     input:
#         gtf = join(out_dir,'07_gtf', build_id + '_talon.gtf'),
#         counts = join(out_dir,'06_read_counts', build_id + '_talon_abundance_filtered.tsv')
#     params:
#         rname = "11_sqanti",
#         anno = anno_gtf + " " + anno_fa,
#         gff = anno_gff
#     container: "docker://nciccbr/ccbr_sqanti_3:latest"
#     output:
#         o1 = join(out_dir,'tbd')
#     shell:
#         '''
#         sqanti3_qc.py --gtf /data/sevillas2/rbl3/07_gtf/SIRV_talon.gtf /data/CCBR_Pipeliner/db/PipeDB/Indices/GTFs/hg38/gencode.v30.annotation.gtf \
#             /data/CCBR_Pipeliner/db/PipeDB/Indices/hg38_30/ref.fa --fl_count /data/sevillas2/rbl3/06_read_counts/SIRV_talon_abundance_filtered.tsv \
#             --isoAnnotLite --gff3 /data/CCBR/projects/rbl3/dependencies/Homo_sapiens_GRCh38_Ensembl_86.gff3
#         '''

# '''
# rule :
#     input:
#         f1 = 
#     params:
#         rname = "create_sam",
#     envmodules:
#         config[''],
#     output:
#         o1 = join(out_dir,)
#     shell:
# '''


