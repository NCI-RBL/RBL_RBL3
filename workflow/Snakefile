from os.path import join
import pandas as pd
from collections import defaultdict
import yaml
import csv

#config dir
source_dir = config['sourceDir']
out_dir = config['outputDir'].rstrip('/') + '/'
fastq_dir = config['fastqDir'].rstrip('/') + '/'
sample_manifest=config['sampleManifest']
deg_manifest = config['DEGManifest']

singularity_parameter = 'singularity exec -B /data,/fdb,/scratch,/lscratch,' + out_dir + ',' + source_dir
talon_conda = config['condaDir'] + config['talonConda']
porechop_conda = config['condaDir'] + config['porechopConda']
flair_conda = config['condaDir'] + config['flairConda']

#config user params
masked_refs=config['maskedReference']
clean_up=config['cleanTranscripts']
annotation_id=config['annotationID']
build_id=config['buildID']
maxFracA=config['maxFracA']
minCount=config['minCount']
minDatasets=config['minDatasets']
platform_id=config['platformID']
primer_len=config['primerLength']
perc_sim = config['percentSimilarity']
num_match = config['numberMatches']

###############################################################
# functions
###############################################################
#annotation FA with wildcards
#if wildcards are used for bc, check if masking is required 
def get_annotation_fa(wildcards):
    if hasattr(wildcards, 'masked_flag'):
        if (wildcards.masked_flag == "masked"):
            anno_file=join(out_dir,"00_tmp","annotation_files","masked.fa")
        else:
            anno_file=join(out_dir,"00_tmp","annotation_files","unmasked.fa")
    else:
        if (masked_refs == "Y"):
            anno_file=join(out_dir,"00_tmp","annotation_files","masked.fa")
        else:
            anno_file=join(out_dir,"00_tmp","annotation_files","unmasked.fa")
    return(anno_file)

#input for bam dependent on whether cleanup is run
def get_bam_input(wildcards):
    if (clean_up=="Y"):
        bam_input = join(out_dir,'02_sam_corrected',wildcards.bc + '_' + wildcards.masked_flag + '.sorted_clean.sam')
    else:
        bam_input = join(out_dir,'02_sam', wildcards.bc + '_' + wildcards.masked_flag + '.sorted.sam')

    return(bam_input)

#annotation GTF with wildcards
def get_annotation_gtf(wildcards):
    if (masked_refs == "Y"):
        anno_file=join(out_dir,"00_tmp","annotation_files","masked.gtf")
    else:
        anno_file=join(out_dir,"00_tmp","annotation_files","unmasked.gtf")
    return(anno_file)

#talon config
def talon_config(wildcards):
    
    #create array for config
    config_data = []

    #for each of the barcodes, generate config info
    #example: SIRV_Rep1,SIRV,PacBio-Sequel2,/data/sevillas2/RBL3/tutorial/labeled/SIRV_rep1_labeled.sam
    for bc in bc_list:
        df_sub = df_sample[df_sample["filename"] == bc]
        
        output_filename = join(out_dir,'05_talon','sam_labeled',bc + '_labeled.sam')
        config_data.append(df_sub.iloc[0]['sampleid'] + "," + build_id + "," + platform_id + "," + output_filename)
    
    #create config
    talon_config = join(out_dir,'05_talon', 'talon_config.csv')
    with open(talon_config, "w") as txt_file:
        for line in config_data:
            txt_file.write(line + "\n")

#input for talon_prime dependent on whether cleanup is run -  will only run masked if that is present
def get_talon_input(wildcards):
    if (clean_up=="Y"):
        if (masked_refs=="N"):
            talon_input = join(out_dir,'02_sam_corrected',wildcards.bc + '_unmasked.sorted_clean.sam')
        else:
            talon_input = join(out_dir,'02_sam_corrected',wildcards.bc + '_masked.sorted_clean.sam')
    else:
        if (masked_refs=="N"):
            talon_input = join(out_dir,'02_sam',wildcards.bc + '_unmasked.sorted.sam')
        else:
            talon_input = join(out_dir,'02_sam',wildcards.bc + '_masked.sorted.sam')
    return(talon_input)

#flair config
def flair_config(wildcards):
    
    #create array for config
    config_data = []

    #for each of the barcodes, generate config info
    #sampleid condition batchid fastqfile
    #example: wt1	wt	b1	/scratch/kopardevn/rbl3_test_out/fastq/barcode01.trimmed.R1.fastq.gz
    for bc in bc_list:
        df_sub = df_sample[df_sample["filename"] == bc]
        
        fq_name = join(out_dir,'06_flair','fastq', bc + '.fastq')
        config_data.append(df_sub.iloc[0]['sampleid'] + "\t" + df_sub.iloc[0]['groupid'] + "\t" + df_sub.iloc[0]['batchid'] + "\t" + fq_name)
    
    #create config
    flair_config = join(out_dir,'06_flair', 'flair_config.csv')
    with open(flair_config, "w") as txt_file:
        for line in config_data:
            txt_file.write(line + "\n")

def get_deg_list():
    list_path=deg_manifest
    
    #read file
    with open(list_path) as f:
        reader = csv.reader(f, delimiter="\t")
        deg_file = list(reader)

        #for each group in deg list
        deg_list=[]
        for group in deg_file:
            
            #for each individual id
            for id in group:
                id = id.replace(",", "_")
            
            #append final list
            deg_list.append(id)
        
        #remove header
        deg_list.pop(0)

    return(deg_list)

def get_deg_comparisons(wildcards):

  #create split sample ids
  id1 = wildcards.group_id.split("_")[0]
  id2 = wildcards.group_id.split("_")[1]

  #create group ids
  sub_df = df_sample[df_sample['sampleid']==id1]
  group1 = id1 + "_" + sub_df.iloc[0]['groupid'] + "_" + sub_df.iloc[0]['batchid']
    
  sub_df = df_sample[df_sample['sampleid']==id2]
  group2 = id2 + "_" + sub_df.iloc[0]['groupid'] + "_" + sub_df.iloc[0]['batchid']

  #create merged groupid
  groupid = group1 + " " + group2

  return (groupid)
###############################################################
# main code
###############################################################
#barcode list
df_sample = pd.read_csv(sample_manifest,sep="\t")
bc_list = df_sample['filename']

#masked list
if (masked_refs =="Y"):
    masked_list=["masked","unmasked"]
else:
    masked_list=["unmasked"]

#deg manifest
deg_list = get_deg_list()

###############################################################
# rule all
###############################################################
#local rules
localrules: talon_config, flair_config, flair_fastq

#annotation file output will differ depending on masking of reference files
if (masked_refs=="Y"):
    input_annotation = [join(out_dir,"00_tmp","annotation_files","unmasked.gtf"),
                        join(out_dir,"00_tmp","annotation_files","unmasked.fa"),
                        join(out_dir,"00_tmp","annotation_files","masked.gtf"),
                        join(out_dir,"00_tmp","annotation_files","masked.fa")]
else:
    input_annotation = [join(out_dir,"00_tmp","annotation_files","unmasked.gtf"),
                        join(out_dir,"00_tmp","annotation_files","unmasked.fa")]

#sam files output will differ depending on cleanup
if clean_up=="Y":
    input_sam = [expand(join(out_dir,'02_sam','{bc}_{masked_flag}.sorted.sam'),bc=bc_list, masked_flag=masked_list),
                expand(join(out_dir,'02_sam_corrected','{bc}_{masked_flag}.sorted_clean.sam'),bc=bc_list, masked_flag=masked_list)]
else:
    input_sam = [expand(join(out_dir,'02_sam','{bc}_{masked_flag}.sorted.sam'),bc=bc_list, masked_flag=masked_list)]

rule all:
    input:
        #input annotation files
        input_annotation,

        # #input fastq files
        # expand(join(fastq_dir,'{bc}.fastq'),bc=bc_list),
        # expand(join(out_dir,'01_fastq','{bc}.fastq.gz'),bc=bc_list),
        # expand(join(out_dir,'01_fastq_trimmed','{bc}.fastq.gz'),bc=bc_list),

        # #sam files
        # input_sam,

        #bam files
        expand(join(out_dir,'03_bam','{bc}_{masked_flag}.sorted.bam'),bc=bc_list, masked_flag=masked_list),

        #qc
        # expand(join(out_dir, '04_qc','fastqc','{bc}_fastqc.html'), bc=bc_list),
        # expand(join(out_dir, '04_qc','samtools','{bc}_{masked_flag}_samstats.txt'), bc=bc_list, masked_flag=masked_list),
        join(out_dir,'04_qc','multiqc_report.html'),
        # expand(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_align_len.txt'), bc=bc_list, masked_flag=masked_list),
        join(out_dir,'04_qc','qc_report.html'),
        
        #talon
        join(out_dir,'05_talon', 'talon_config.csv'),
        # join(out_dir,'05_talon', build_id + '.db'),
        # expand(join(out_dir,'05_talon','sam_labeled','{bc}_labeled.sam'),bc=bc_list),
        join(out_dir,'05_talon','annotate', build_id + '_talon_read_annot.tsv'),
        join(out_dir,'05_talon','counts', build_id + '_talon_summary.tsv'),
        join(out_dir,'05_talon','counts', build_id + '_talon_abundance.tsv'),
        join(out_dir,'05_talon','counts', build_id + '_whitelist.txt'),
        join(out_dir,'05_talon','counts', build_id + '_talon_abundance_filtered.tsv'),
        join(out_dir,'05_talon','gtf', build_id + '_talon.gtf'),
        expand(join(out_dir,'05_talon','transcript_filtered','{bc}_category_list.txt'),bc=bc_list),
        
        #flair
        # join(out_dir,'06_flair','merged.fastq.gz'),
        # join(out_dir,'06_flair','isoforms','merged_flair.isoforms.fa'),
        join(out_dir,'06_flair','flair_config.csv'),
        # expand(join(out_dir,'06_flair','fastq','{bc}.fastq'),bc=bc_list),
        join(out_dir,'06_flair','counts','flair_counts_matrix.tsv'),
        expand(join(out_dir,'07_deg','deg_iso_{group_id}.txt'),group_id=deg_list),

        #report
        join(out_dir,'08_report','summary_report.html'),

        # #squanti
        # #join(out_dir,'tbd'),

#common and other SMK 
if source_dir == "":
    include: "rules/common.smk"
else:
    include: join(source_dir,"workflow/rules/common.smk")

###############################################################
# snakemake rules
###############################################################
rule copy_annotation_unmasked:
    '''
    create local instance of annotation files to be removed upon pipeline completion
    '''
     input:
        fa = config['annotationFA'],
        gtf = config['annotationGTF']
    params:
        rname = "01_copy_annotation_unmasked",
        base = join(out_dir,"00_annotation_files")
    output:
        fa = temp(join(out_dir,"00_tmp","annotation_files","unmasked.fa")),
        gtf = temp(join(out_dir,"00_tmp","annotation_files","unmasked.gtf"))
    shell:
        """
        cp {input.fa} {output.fa}; \
        cp {input.gtf} {output.gtf}
        """

if (masked_refs=="Y"):
    rule copy_annotation_masked:
        '''
        create local instance of annotation files to be removed upon pipeline completion
        '''
        input:
            fa = config['annotationFAmasked'],
            gtf = config['annotationGTFmasked']
        params:
            rname = "01_copy_annotation_masked",
            base = join(out_dir,"00_annotation_files")
        output:
            fa = temp(join(out_dir,"00_tmp","annotation_files","masked.fa")),
            gtf = temp(join(out_dir,"00_tmp","annotation_files","masked.gtf"))
        shell:
            """
            cp {input.fa} {output.fa}; \
            cp {input.gtf} {output.gtf}
            """

rule handle_fastq:
    '''
    move and zip fastq files 
    '''
    input:
        f1 = join(fastq_dir,'{bc}.fastq')
    params:
        rname = "01_fq",
    output:
        zip = temp(join(out_dir,'01_fastq','{bc}.fastq.gz'))
    shell:
        """
        gzip -c {input.f1} > {output.zip}
        """

rule adaptor_trim:
    '''
    adapter trimming
    '''
    input:
        f1 = join(out_dir,'01_fastq','{bc}.fastq.gz')
    params:
        rname = "01_fq_trimadaptors",
        sing_param = singularity_parameter, 
        doc = porechop_conda,
    envmodules:
        config['singularity'],
    output:
        o1 = temp(join(out_dir,'01_fastq_trimmed','{bc}.fastq.gz'))
    shell:
        '''
        {params.sing_param} {params.doc} porechop -i {input.f1} -o {output.o1} -t 2
        '''

rule create_sam:
    '''
    # cupcake tutorial
    #https://github.com/Magdoll/cDNA_Cupcake/wiki/Cupcake:-supporting-scripts-for-Iso-Seq-after-clustering-step

    minimap flags
    https://lh3.github.io/minimap2/minimap2.html
    '''
    input:
        f1 = join(out_dir,'01_fastq_trimmed','{bc}.fastq.gz')
    params:
        rname = "02_sam",
        anno_fa = get_annotation_fa
    envmodules:
        config['minimap2'],
        config['samtools']
    output:
        sam = temp(join(out_dir,'02_sam','{bc}_{masked_flag}.sam')),
        sorted = temp(join(out_dir,'02_sam','{bc}_{masked_flag}.sorted.sam'))
    shell:
        '''
        minimap2 \
        -ax splice -t 30 --secondary=no --MD \
        {params.anno_fa} {input.f1} > {output.sam};
        samtools sort {output.sam} -o {output.sorted}
        '''

if (clean_up=="Y"):
    rule clean_sam:
        '''
        https://github.com/mortazavilab/TranscriptClean

        Corrects mismatches, microindels, and noncanonical splice junctions in long reads that have been mapped to the genome 
        '''
        input:
            f1 = join(out_dir,'02_sam','{bc}_{masked_flag}.sorted.sam')
        params:
            rname = "02_sam_corrected",
            sing_param = singularity_parameter, 
            doc = talon_conda,
            anno_fa = get_annotation_fa,
            base = join(out_dir,'02_sam_corrected','{bc}_{masked_flag}.sorted'),
            tmp_dir = join(out_dir,'00_tmp','sam_corrected','tmp_{bc}_{masked_flag}/')
        envmodules:
            config['singularity'],
        output:
            o1 = temp(join(out_dir,'02_sam_corrected','{bc}_{masked_flag}.sorted_clean.sam'))
        shell:
            '''
            {params.sing_param} {params.doc} TranscriptClean.py --sam {input.f1} --genome {params.anno_fa} \
            --tmpDir {params.tmp_dir} --outprefix {params.base};
            '''

rule create_bam:
    input:
        f1 = get_bam_input
    params:
        rname='03_bam',
    envmodules:
        config['samtools']
    output:
        bam = temp(join(out_dir,'03_bam','{bc}_{masked_flag}.bam')),
        sorted = join(out_dir, '03_bam','{bc}_{masked_flag}.sorted.bam')
    shell:
        """
        samtools view -bS {input.f1} -o {output.bam}; \
        samtools sort {output.bam} -o {output.sorted}; \
        samtools index {output.sorted}; 
        """

rule qc_fastq:
    """
    Runs FastQC report on each sample 
    """
    input:
        fq = join(out_dir,'01_fastq_trimmed','{bc}.fastq.gz')
    params:
        rname='04_fqc',
        base = join(out_dir, '04_qc','fastqc'),
    envmodules:
        config['fastqc']
    output:
        o1 = temp(join(out_dir, '04_qc','fastqc','{bc}_fastqc.html'))
    shell:
        """
        fastqc {input.fq} -o {params.base}
        """

rule qc_samstats:
    """
    generate statistics for sam file before deduplication
    http://www.htslib.org/doc/samtools-stats.html
    > $1
    """
    input:
        f1 = join(out_dir, '03_bam','{bc}_{masked_flag}.sorted.bam')
    params:
        rname='04_samstats'
    envmodules:
        config['samtools']
    output:
        o1 = temp(join(out_dir, '04_qc','samtools','{bc}_{masked_flag}_samstats.txt'))
    shell:
        """
        samtools view -h {input.f1} | samtools stats - > {output.o1}
        """

rule qc_multiqc:
    """
    merges FastQC reports for pre/post trimmed fastq files into MultiQC report
    https://multiqc.info/docs/#running-multiqc
    """
    input:
        f1 = expand(join(out_dir, '04_qc', 'fastqc','{bc}_fastqc.html'),bc=bc_list),
        f2 = expand(join(out_dir, '04_qc', 'samtools','{bc}_{masked_flag}_samstats.txt'),bc=bc_list, masked_flag=masked_list)
    params:
        rname = '04_multiqc',
        out = join(out_dir,'04_qc'),
        qc_config = join(source_dir,'config','multiqc_config.yaml'),
        dir_fastq = join(out_dir, '04_qc', 'fastqc'),
        dir_sam = join(out_dir, '04_qc', 'samtools'),
    envmodules:
        config['multiqc']
    output:
        o1 = join(out_dir,'04_qc','multiqc_report.html')
    shell:
        """
        multiqc -f -v -c {params.qc_config} \
        -d -dd 1 {params.dir_fastq} {params.dir_sam} \
        -o {params.out}
        """

rule qc_alignment:
    """
    uses samtools to create a bams of unaligned reads and aligned reads
    input; print qlength col to text file
    generates plots and summmary file for aligned vs unaligned statistics
    """
    input:
        f1 = join(out_dir, '03_bam','{bc}_{masked_flag}.sorted.bam')
    params:
        rname = "04_qc_align",
        R = join(source_dir,'workflow','scripts','01_alignment_stats.R'),
        base = join(out_dir, '04_qc', 'alignment/')
    envmodules:
        config['samtools'],
        config['R']
    output:
        bam_a = temp(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_align_len.txt')),
        bam_u = temp(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_unalign_len.txt')),
        png_align = temp(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_aligned.png')),
        png_unalign = temp(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_unaligned.png')),
        txt_align = temp(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_aligned.txt')),
        txt_unalign = temp(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_unaligned.txt')),
    shell:
        """
        samtools view -F 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_a}; \
        samtools view -f 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_u}; \
        Rscript {params.R} "{wildcards.bc}_{wildcards.masked_flag}" {output.bam_a} {output.bam_u} {params.base}
        """

if (masked_refs == "Y"):
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_aligned.png'), bc=bc_list, masked_flag=masked_list),
            png_unalign = expand(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_unaligned.png'), bc=bc_list, masked_flag=masked_list),
            txt_align = expand(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_aligned.txt'), bc=bc_list, masked_flag=masked_list),
            txt_unalign = expand(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_unaligned.txt'), bc=bc_list, masked_flag=masked_list),
        params:
            rname = "04_qc_report",
            png_align = expand(join(out_dir, '04_qc', 'alignment','{bc}_masked_aligned.png'), bc=bc_list),
            png_unalign = expand(join(out_dir, '04_qc', 'alignment','{bc}_masked_unaligned.png'), bc=bc_list),
            txt_align = expand(join(out_dir, '04_qc', 'alignment','{bc}_masked_aligned.txt'), bc=bc_list),
            txt_unalign = expand(join(out_dir, '04_qc', 'alignment','{bc}_masked_unaligned.txt'), bc=bc_list),
            R = join(source_dir,'workflow','scripts','02_qc_report_comparemasking.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'04_qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{params.txt_align}", \
                u_txt = "{params.txt_unalign}"))'
            """
else:
    rule qc_troubleshoot:
            """
            generates a PDF of barcode plots and alignment plots for qc troubleshooting
            """
            input:
                png_align = expand(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_aligned.png'), bc=bc_list, masked_flag=masked_list),
                png_unalign = expand(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_unaligned.png'), bc=bc_list, masked_flag=masked_list),
                txt_align = expand(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_aligned.txt'), bc=bc_list, masked_flag=masked_list),
                txt_unalign = expand(join(out_dir, '04_qc', 'alignment','{bc}_{masked_flag}_unaligned.txt'), bc=bc_list, masked_flag=masked_list),
            params:
                rname = "04_qc_report",
                R = join(source_dir,'workflow','scripts','02_qc_report_unmasked.Rmd'),
            envmodules:
                config['R']
            output:
                o1 = join(out_dir,'04_qc','qc_report.html')
            shell:
                """
                Rscript -e 'library(rmarkdown); \
                rmarkdown::render("{params.R}",
                    output_file = "{output.o1}", \
                    params= list(a_txt = "{input.txt_align}", \
                        u_txt = "{input.txt_unalign}"))'
                """

rule talon_config:
    '''
    generate talon config
    '''
    input:
        f1 = expand(join(out_dir,'01_fastq_trimmed','{bc}.fastq.gz'),bc=bc_list),
    params:
        process = talon_config
    output:
        o1 = join(out_dir,'05_talon','talon_config.csv')

rule talon_db:
    '''
    initialize db
    '''
    input:
        fa = join(out_dir,"00_tmp","annotation_files","unmasked.fa"),
        gtf = join(out_dir,"00_tmp","annotation_files","unmasked.gtf"),
    params:
        rname = "05.1_talon_db",
        sing_param = singularity_parameter, 
        doc = talon_conda,
        a_id = annotation_id,
        b_id = build_id,
        anno_gtf = get_annotation_gtf,
        base = join(out_dir,'05_talon',build_id),
    output:
        o1 = temp(join(out_dir,'05_talon', build_id + '.db'))
    envmodules:
        config['singularity'],
    shell:
        '''
        {params.sing_param} {params.doc} talon_initialize_database \
            --f {params.anno_gtf} \
            --a {params.a_id}\
            --g {params.b_id} \
            --o {params.base}
        '''

rule talon_prime:
    '''
    Current long-read platforms that rely on poly-(A) selection are prone to internal priming artifacts. 
    These occur when the oligo-dT primer binds off-target to A-rich sequences inside an RNA transcript 
    rather than at the end. 
    
    Records the fraction of As in the n-sized window immediately following each read alignment; output SAM 
    file with the fraction recorded in the fA:f custom SAM tag
    
    tmp_dir needs to be unique to sample; otherwise run will fail as it attempts to write over
    files with each sample
    --deleteTmp
    '''
    input:
        f1 = get_talon_input,
        db = join(out_dir,'05_talon', build_id + '.db')
    params:
        rname = "05.2_talon_prime",
        sing_param = singularity_parameter, 
        doc = talon_conda,
        anno_fa = get_annotation_fa,
        tmp_dir = join(out_dir,"00_tmp","talon_prime", '{bc}'),
        base_sample = join(out_dir,'05_talon','sam_labeled','{bc}'),
        p_len = primer_len, 
    output:
        o1 = temp(join(out_dir,'05_talon','sam_labeled','{bc}_labeled.sam')),
        o2 = temp(join(out_dir,'05_talon','sam_labeled','{bc}_read_labels.tsv')),
    envmodules:
        config['singularity'],
    shell:
        '''
        {params.sing_param} {params.doc} talon_label_reads --f {input.f1} \
            --g {params.anno_fa} \
            --t 1 \
            --ar {params.p_len} \
            --tmpDir={params.tmp_dir} \
            --o {params.base_sample}
        '''

rule talon_annotation:
    '''
    annotate and quantify reads; modify db
    '''
    input:
        db = join(out_dir, '05_talon', build_id + '.db'),
        t_config = join(out_dir, '05_talon', 'talon_config.csv'),
        sam = expand(join(out_dir,'05_talon','sam_labeled','{bc}_labeled.sam'),bc=bc_list)
    params:
        rname = "05.3_annotate",
        sing_param = singularity_parameter, 
        doc = talon_conda,
        b_id = build_id,
        tmp_dir = join(out_dir,'00_tmp','talon_annotations/'),
        base_sample = join(out_dir,'05_talon','annotate',build_id),
    output:
        o1 = join(out_dir,'05_talon','annotate', build_id + '_talon_read_annot.tsv'),
        o2 = join(out_dir,'05_talon','annotate', build_id + '_QC.log'),
    envmodules:
        config['singularity'],
    shell:
        '''
        {params.sing_param} {params.doc} talon \
            --f {input.t_config} \
            --db {input.db} \
            --build {params.b_id} \
            --threads 5 \
            --tmp_dir {params.tmp_dir} \
            --o {params.base_sample}
        '''

rule talon_summary:
    '''
    summarize how many transcripts before filtering
    '''
    input:
        db = join(out_dir,'05_talon',build_id + '.db'),
        anno = join(out_dir,'05_talon','annotate', build_id + '_talon_read_annot.tsv'),
    params:
        rname = "05.4_count_summary",
        sing_param = singularity_parameter, 
        doc = talon_conda,
        base = join(out_dir,'05_talon','counts', build_id),
    output:
        o1 = join(out_dir,'05_talon','counts', build_id + '_talon_summary.tsv')
    envmodules:
        config['singularity'],
    shell:
        '''
        {params.sing_param} {params.doc} talon_summarize --db {input.db} --v --o {params.base}
        '''

rule talon_counts:
    '''
    abundance matrix (for comp gene expression) without filtering
    '''
    input:
        db = join(out_dir,'05_talon',build_id + '.db'),
        anno = join(out_dir,'05_talon','annotate', build_id + '_talon_read_annot.tsv'),
    params:
        rname = "05.6_count_abund",
        sing_param = singularity_parameter, 
        doc = talon_conda,
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'05_talon','counts', build_id),
    output:
        o1 = join(out_dir,'05_talon','counts', build_id + '_talon_abundance.tsv')
    envmodules:
        config['singularity'],
    shell:
        '''
        {params.sing_param} {params.doc} talon_abundance --db {input.db} -a {params.a_id} --build {params.b_id} --o {params.base}
        '''

rule talon_whitelist:
    '''
    repeat with TALON filters
    '''
    input:
        db = join(out_dir,'05_talon',build_id + '.db'),
        anno = join(out_dir,'05_talon','annotate', build_id + '_talon_read_annot.tsv'),
    params:
        rname = "05.7_count_whitelist",
        sing_param = singularity_parameter, 
        doc = talon_conda,
        a_id = annotation_id,
        max_frac  = maxFracA,
        min_count = minCount,
        min_ds = minDatasets,
    output:
        o1 = join(out_dir,'05_talon','counts',build_id + '_whitelist.txt')
    envmodules:
        config['singularity'],
    shell:
        '''
        {params.sing_param} {params.doc} talon_filter_transcripts \
            --db {input.db} \
            -a {params.a_id} \
            --maxFracA {params.max_frac} \
            --minCount {params.min_count} \
            --minDatasets {params.min_ds} \
            --o {output.o1}
        '''

rule talon_abundance_filtered:
    '''
    abundance matrix (for comp gene expression) with filtering
    '''
    input:
        db = join(out_dir,'05_talon',build_id + '.db'),
        white_list = join(out_dir,'05_talon','counts',build_id + '_whitelist.txt')
    params:
        rname = "05.8_counts_filt",
        sing_param = singularity_parameter, 
        doc = talon_conda,
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'05_talon','counts', build_id),
    output:
        o1 = join(out_dir,'05_talon','counts', build_id + '_talon_abundance_filtered.tsv')
    envmodules:
        config['singularity'],
    shell:
        '''
        {params.sing_param} {params.doc} talon_abundance \
            --db {input.db} \
            -a {params.a_id} \
            --whitelist {input.white_list} \
            --build {params.b_id} \
            --o {params.base}
        '''

rule talon_gtf:
    '''
    create custom GTF of filtered transcripts
    '''
    input:
        db = join(out_dir,'05_talon',build_id + '.db'),
        w_list = join(out_dir,'05_talon','counts',build_id + '_whitelist.txt')
    params:
        rname = "05.9_gtf",
        sing_param = singularity_parameter, 
        doc = talon_conda,
        a_id = annotation_id,
        b_id = build_id,
        base = join(out_dir,'05_talon','gtf', build_id),
    output:
        o1 = join(out_dir,'05_talon','gtf', build_id + '_talon.gtf')
    envmodules:
        config['singularity'],
    shell:
        '''
        {params.sing_param} {params.doc} talon_create_GTF \
            --db {input.db} \
            --whitelist {input.w_list} \
            -a {params.a_id} \
            --build {params.b_id} \
            --o {params.base}
        '''

if (masked_refs == "Y"):
    rule create_masked_outputs:
        '''
        http://broadinstitute.github.io/picard/command-line-overview.html#FilterSamReads

        Using talon annotations, BAM files are created for the transcript_novelty category. The following
        are categories that can be found in a sample: Antisense, Genomic, ISM, Known, NIC, NNC
    '''
        input:
            anno = join(out_dir,'05_talon','annotate', build_id + '_talon_read_annot.tsv'),
            sam = join(out_dir,'05_talon','sam_labeled','{bc}_labeled.sam')
        params:
            rname = "todo",
            base = join(out_dir,'05_talon','transcript_filtered','{bc}_')
        envmodules:
            config['java'],
            config['picard'],
            config['samtools'],
            config['bedtools']
        output:
            bam = temp(join(out_dir,'05_talon','transcript_filtered','{bc}_labeled.bam')),
            cat_list = temp(join(out_dir,'05_talon','transcript_filtered','{bc}_category_list.txt')),
        shell:
            """
            #determine which transcript categories are present in samples, skip header
            awk '(NR>1)' {input.anno} | cut -f17 - | sort | uniq > {output.cat_list};

            #create bam
            samtools view -bS {input.sam}> {output.bam};

            #create read list, create subset bam file, sam file 
            while read p; do \
                cat {input.anno} | awk -v p="$p" '$17 == '"p"'' | cut -f1 > {params.base}${{p}}_readlist.txt;

                java -jar $PICARDJARPATH/picard.jar FilterSamReads \
                    I={output.bam} \
                    O={params.base}${{p}}.bam \
                    READ_LIST_FILE={params.base}${{p}}_readlist.txt \
                    FILTER=includeReadList;

                samtools view -h -o {params.base}${{p}}.sam {params.base}${{p}}.bam;

                bedtools bamtofastq -i {params.base}${{p}}.bam -fq {params.base}${{p}}.fastq;
            done <{output.cat_list};
  
            """

rule merge_fq:
    '''
    '''
    input:
        fqs = expand(join(out_dir,'01_fastq_trimmed','{bc}.fastq.gz'),bc=bc_list)
    params:
        rname = "06.1_flair_merge",
        base = join(out_dir,'01_fastq_trimmed')
    output:
        o1 = temp(join(out_dir,'06_flair','merged.fastq.gz'))
    shell:
        '''
        zcat {params.base}/*.fastq.gz | gzip -n -> {output.o1}
        '''

rule flair_isoforms:
    '''
    run flair to create isoforms
    
    singularity exec -B /scratch/kopardevn,/data/CCBR_Pipeliner /scratch/kopardevn/rbl3_test_out/ccbr_flair_v1.5.sif \
        flair.py 123 -g /data/CCBR_Pipeliner/db/PipeDB/Indices/hg38_basic/indexes/hg38.fa \
        -r  /scratch/kopardevn/rbl3_test_out/fastq/merged.fastq.gz \
        -f /data/CCBR_Pipeliner/db/PipeDB/Indices/GTFs/hg38/gencode.v30.annotation.gtf \
        -o /scratch/kopardevn/rbl3_test_out/fastq/merged.flair.output \
        --threads 16 \
        --temp_dir /scratch/kopardevn/rbl3_test_out/fastq/merged.flair.output.tmpdir

    -f and temp are not options - todo REMOVE
    Align unrecognized arguments: -f /data/CCBR/projects/rbl3/dependencies/gencode.v30.annotation.gtf --temp_dir /scratch/sevillas2/rbl3_flair/06_flair/tmp_iso
    '''
    input:
        f1 = join(out_dir,'06_flair','merged.fastq.gz')
    params:
        rname = "06.2_flair_isoforms",
        sing_param = singularity_parameter, 
        doc = flair_conda,
        anno_fa = get_annotation_fa,
        anno_gtf = get_annotation_gtf,
        base = join(out_dir,'06_flair','isoforms','merged_flair'),
        tmp_dir = join(out_dir,'00_tmp','tmp_iso'),
    envmodules:
        config['singularity'],
    output:
        o1 = temp(join(out_dir,'06_flair','isoforms','merged_flair.isoforms.fa')),
        o2 = temp(join(out_dir,'06_flair','isoforms','merged_flair.isoforms.gtf')),
        o3 = temp(join(out_dir,'06_flair','isoforms','merged_flair_all_corrected.bed')),
        o4 = temp(join(out_dir,'06_flair','isoforms','merged_flair_all_inconsistent.bed')),
        o5 = temp(join(out_dir,'06_flair','isoforms','merged_flair.bam')),
        o6 = temp(join(out_dir,'06_flair','isoforms','merged_flair.bed')),
        o7 = temp(join(out_dir,'06_flair','isoforms','merged_flair.isoforms.bed')),
        o8 = temp(join(out_dir,'06_flair','isoforms','merged_flair.sam')),
    shell:
        '''
        {params.sing_param} {params.doc} flair.py 123 -g {params.anno_fa} \
            -r  {input.f1} \
            -f {params.anno_gtf} \
            -o {params.base} \
            --threads 16 \
            --temp_dir {params.tmp_dir}
        '''

rule flair_config:
    '''
    generate flair config
    '''
    input:
        f1 = expand(join(out_dir,'01_fastq_trimmed','{bc}.fastq.gz'),bc=bc_list),
    params:
        process = flair_config
    output:
        o1 = join(out_dir,'06_flair','flair_config.csv')

rule flair_fastq:
    '''
    unzip fastq files for flair input
    '''
    input:
        f1 = join(out_dir,'01_fastq_trimmed','{bc}.fastq.gz')
    params:
        zip = join(out_dir,'06_flair','fastq','{bc}.fastq.gz')
    output:
        o1 = temp(join(out_dir,'06_flair','fastq','{bc}.fastq'))
    shell:
        '''
        cp {input.f1} {params.zip}; \
        gunzip {params.zip}
        '''

rule flair_abundances:
    '''
    ## quantify

    singularity exec -B /scratch/kopardevn,/data/CCBR_Pipeliner /scratch/kopardevn/rbl3_test_out/ccbr_flair_v1.5.sif \
    flair.py quantify \
    -r /scratch/kopardevn/rbl3_test_out/fastq/reads_manifest.tsv \
    --threads 16 \
    --tpm \
    -i /scratch/kopardevn/rbl3_test_out/fastq/merged.flair.output.isoforms.fa \
    --temp_dir /scratch/kopardevn/rbl3_test_out/fastq/flair.quantify.tmpdir
    '''
    input:
        config = join(out_dir,'06_flair','flair_config.csv'),
        iso = join(out_dir,'06_flair','isoforms','merged_flair.isoforms.fa'),
        fastq = expand(join(out_dir,'06_flair','fastq','{bc}.fastq'),bc=bc_list)
    params:
        rname = "06.3_flair_abundances",
        sing_param = singularity_parameter, 
        docs = flair_conda,
        tmp_dir = join(out_dir,'00_tmp','tmp_abund'),
        base = join(out_dir,'06_flair','counts','flair_counts_matrix.tsv')
    envmodules:
        config['singularity'],
    output:
        o1 = join(out_dir,'06_flair','counts','flair_counts_matrix.tsv')
    shell:
        '''
        {params.sing_param} {params.docs} flair.py quantify \
            -i {input.iso} \
            -r {input.config} \
            -o {params.base} \
            --temp_dir {params.tmp_dir} \
            --tpm \
            --threads 16 \
        '''

rule flair_deg:
    '''
    #using the diff_iso_usage.py script instead

    singularity exec -B /scratch/kopardevn,/data/CCBR_Pipeliner /scratch/kopardevn/rbl3_test_out/ccbr_flair_v1.5.sif \
    bash -c "python3 /opt2/flair/bin/diff_iso_usage.py /scratch/kopardevn/rbl3_test_out/fastq/counts_matrix.tsv 
    wt1_wt_b1 ko_drosha1_ko_drosha_b1 /scratch/kopardevn/rbl3_test_out/fastq/diff_iso_ko_drosha.txt"

    alternative diff is for projects with samples > 2 per group
    - python flair.py diffExp -q counts_matrix.tsv -o output_directory [options]
    - {params.sing_param} {params.docs} flair.py diffExp -q {input.f1} -o {params.base}
    '''
    input:
        f1 = join(out_dir,'06_flair','counts','flair_counts_matrix.tsv')
    params:
        rname = "06.4_flair_deg",
        sing_param = singularity_parameter, 
        docs = flair_conda,
        groupid = get_deg_comparisons,
        base = join(out_dir,'07_deg')
    envmodules:
        config['singularity'],
        config['python'], 
    output:
        o1 = join(out_dir,'07_deg','deg_iso_{group_id}.txt')
    shell:
        '''
        mkdir -p {params.base};
        {params.sing_param} {params.docs} bash -c "python3 /opt2/flair/bin/diff_iso_usage.py {input.f1} {params.groupid} {output.o1}"
        '''

rule final_report:
    input:
        unfilt = join(out_dir,'05_talon','counts', build_id + '_talon_abundance.tsv'),
        filt = join(out_dir,'05_talon','counts', build_id + '_talon_abundance_filtered.tsv'),
        flair = join(out_dir,'06_flair','counts','flair_counts_matrix.tsv'),
        degs = expand(join(out_dir,'07_deg','deg_iso_{group_id}.txt'),group_id=deg_list)
    params:
        rname = "07_final_report",
        R = join(source_dir,"workflow","scripts","03_transcript_types.Rmd"),
        base = join(out_dir,'08_report'),
        log = join(out_dir,'log'),
        perc_sim = perc_sim,
        ref = masked_refs,
        clean = clean_up,
        num_match = num_match
    envmodules:
        config['R'],
    output:
        o1 = join(out_dir,'08_report','summary_report.html'),
        o2 = join(out_dir,'08_report','img','filtered_abundance.png')
    shell:
        '''
        Rscript -e 'library(rmarkdown); \
        rmarkdown::render("{params.R}",
            output_file = "{output.o1}", \
            params= list(f_talon_data = "{input.filt}", \
                u_talon_data = "{input.unfilt}", \
                u_flair_data = "{input.flair}", \
                output_dir = "{params.base}", \
                log_dir = "{params.log}", \
                perc_sim = "{params.perc_sim}", \
                ref_masking = "{params.ref}", \
                clean_up = "{params.clean}", \
                num_match = "{params.num_match}", \
                deg_list = "{input.degs}"))'
        '''